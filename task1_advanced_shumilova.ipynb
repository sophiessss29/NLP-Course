{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzoJNcPGyRPoK1sPsmBZEy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophiessss29/NLP-Course/blob/main/task1_advanced_shumilova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "F2iFMuBpFh0s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4wY4FRcFjVD",
        "outputId": "6b119c3a-0bf5-4187-fb5d-e4830175adf6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self, text):\n",
        "\n",
        "        self.text = text\n",
        "        self.tokens = []\n",
        "        self.cleaned_tokens = []\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "I_NMxWa4Fj-F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(self):\n",
        "        \"\"\"\n",
        "        Метод для токенизации текста.\n",
        "        \"\"\"\n",
        "        self.tokens = word_tokenize(self.text)\n",
        "        self.tokens = [word for word in self.tokens if re.match(r'^[a-zA-Z]+$', word)]\n",
        "        return self.tokens\n"
      ],
      "metadata": {
        "id": "qFwSoOPmFoMG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(self):\n",
        "        self.tokenize()\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.cleaned_tokens = [lemmatizer.lemmatize(token) for token in self.cleaned_tokens]\n",
        "        return self.cleaned_tokens\n"
      ],
      "metadata": {
        "id": "EByX0Qi8FqE5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(self):\n",
        "        \"\"\"\n",
        "        Метод для удаления стоп-слов.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        self.cleaned_tokens = [token for token in self.tokens if token.lower() not in stop_words]\n",
        "        return self.cleaned_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "TQ6OPRuuFsa6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O data.txt https://raw.githubusercontent.com/vifirsanova/compling/refs/heads/main/tasks/task1/data.txt\n",
        "# Открываем файл data.txt для чтения в кодировке UTF-8 и сохраняем его содержимое в переменную data\n",
        "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = file.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuPlF-TdFuYI",
        "outputId": "192c599b-8ab4-4051-89f2-f9b1f9674d17"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-05 17:15:42--  https://raw.githubusercontent.com/vifirsanova/compling/refs/heads/main/tasks/task1/data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444304 (434K) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "\rdata.txt              0%[                    ]       0  --.-KB/s               \rdata.txt            100%[===================>] 433.89K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-02-05 17:15:42 (56.0 MB/s) - ‘data.txt’ saved [444304/444304]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor(text)"
      ],
      "metadata": {
        "id": "L22BOsIGGT1e"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = TextProcessor(text)\n",
        "processor.tokenize()\n",
        "processor.lemmatize()\n",
        "processor.remove_stopwords()\n",
        "processor.process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "gRd_ndOvGUth",
        "outputId": "804a3414-f02b-4898-da60-7107ea0e9416"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TextProcessor' object has no attribute 'tokenize'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-bc3211885bfb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TextProcessor' object has no attribute 'tokenize'"
          ]
        }
      ]
    }
  ]
}